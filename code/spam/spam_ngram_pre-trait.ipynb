{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warning\n",
    "data_spam =pd.read_csv(\"spamham.csv\") #lire le fichier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = data_spam[\"spam\"]\n",
    "#print(y)  #instence et classe\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "V = CountVectorizer()\n",
    "X = V.fit_transform(data_spam[\"text\"])\n",
    "#print(X.shape)  #instence et mot\n",
    "\n",
    "## s√©parer train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "#print(X_train.shape[0], X_test.shape[0])  #taille de train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on choisit 3 classifieurs\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "liste_classifieurs= [\n",
    "    [\"Perceptron\", Perceptron(eta0=0.1, random_state=0)],\n",
    "    [\"naive_bayes\", MultinomialNB()],    \n",
    "    [\"Logistic Regression\", LogisticRegression()]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.9837\n",
      "  naive_bayes classifier : 0.9878\n",
      "  Logistic Regression classifier : 0.9884\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.9802\n",
      "  naive_bayes classifier : 0.9907\n",
      "  Logistic Regression classifier : 0.9872\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.9831\n",
      "  naive_bayes classifier : 0.9895\n",
      "  Logistic Regression classifier : 0.9855\n"
     ]
    }
   ],
   "source": [
    "# tester n-gramme\n",
    "\n",
    "en_minuscules,enlever_stopwords  = False, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = CountVectorizer(ngram_range = (min_N, max_N))\n",
    "    X = V.fit_transform(data_spam[\"text\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        clf = algo.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print('  %s classifier : %.4f'%(nom, score))\n",
    "        pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'unigramme est le meilleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords False, Minuscules : False\n",
      "  Perceptron classifier : 0.9855\n",
      "  naive_bayes classifier : 0.9436\n",
      "  Logistic Regression classifier : 0.9034\n",
      "Stopwords False, Minuscules : True\n",
      "  Perceptron classifier : 0.9488\n",
      "  naive_bayes classifier : 0.8901\n",
      "  Logistic Regression classifier : 0.9005\n",
      "Stopwords True, Minuscules : False\n",
      "  Perceptron classifier : 0.9511\n",
      "  naive_bayes classifier : 0.9488\n",
      "  Logistic Regression classifier : 0.9331\n",
      "Stopwords True, Minuscules : True\n",
      "  Perceptron classifier : 0.9506\n",
      "  naive_bayes classifier : 0.9383\n",
      "  Logistic Regression classifier : 0.9255\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#comparer les accuracys des 3 classifieurs en unigramme.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "min_N, max_N = 1, 1\n",
    "\n",
    "for enlever_stopwords in [False, True]:\n",
    "  liste_stopwords = None\n",
    "  if enlever_stopwords==True:\n",
    "    liste_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "    \n",
    "  for en_minuscules in [False, True]:\n",
    "    print(f\"Stopwords {enlever_stopwords}, Minuscules : {en_minuscules}\")\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for max_F in [100]:\n",
    "        V = CountVectorizer(lowercase = en_minuscules, stop_words =  liste_stopwords, max_features = max_F )\n",
    "        X = V.fit_transform(data_spam[\"text\"])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "        for nom, algo in liste_classifieurs:\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules, max_F])\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            score = clf.score(X_test, y_test)\n",
    "print(\"-\"*20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
